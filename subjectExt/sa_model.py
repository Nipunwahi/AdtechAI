# -*- coding: utf-8 -*-
"""SA_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AZkmcwLzLYWSzWcUjt_FZx1AH-IiJTj1
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import load_model
import pickle
import re

dir = "subjectExt/"
pathtodataset = "IMDB Dataset.csv"
glove_file_path =  "drive/My Drive/glove.6B.100d.txt"

class SAModel:
    def __init__(self,isLoaded):
      if isLoaded == True :
        self.model=tf.keras.models.load_model(dir+ 'Training/SA_Model')
        with open(dir + 'Training/tokenizer.pickle', 'rb') as handle:
            self.tokenizer = pickle.load(handle)
      else:
        self.create_model()

    def create_model(self):
        data = pd.read_csv(dir + pathtodataset)
        data = data[['review','sentiment']]
        data['review'] = data['review'].apply(lambda x: x.lower())
        data['review'] = data['review'].apply((lambda x: re.sub('[^a-zA-z0-9\s]','',x)))
        for idx,row in data.iterrows():
          row[0] = row[0].replace('rt',' ')
        max_features = 2000
        self.tokenizer = Tokenizer(num_words=max_features, split=' ')
        self.tokenizer.fit_on_texts(data['review'].values)
        
        X = self.tokenizer.texts_to_sequences(data['review'].values)
        X = pad_sequences(X, maxlen = 100)
        vocab_size = len(self.tokenizer.word_index) + 1
        from numpy import array
        from numpy import asarray
        from numpy import zeros

        embeddings_dictionary = dict()
        glove_file = open(dir + glove_file_path, encoding="utf8")

        for line in glove_file:
            records = line.split()
            word = records[0]
            vector_dimensions = asarray(records[1:], dtype='float32')
            embeddings_dictionary [word] = vector_dimensions
        glove_file.close()

        embedding_matrix = zeros((vocab_size, 100))
        for word, index in self.tokenizer.word_index.items():
            embedding_vector = embeddings_dictionary.get(word)
            if embedding_vector is not None:
                embedding_matrix[index] = embedding_vector

        maxlen = 100
        self.model = tf.keras.Sequential([
         tf.keras.layers.Embedding(vocab_size, 100, weights=[embedding_matrix],input_length = maxlen),
         tf.keras.layers.LSTM(128),
         tf.keras.layers.Dense(2,activation='softmax')                  
        ])
        self.model.compile(loss = "categorical_crossentropy",optimizer = 'adam',metrics = ['accuracy'])
        Y = pd.get_dummies(data['sentiment']).values
        X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)
        batch_size = 128
        self.model.fit(X_train, Y_train, epochs = 6, batch_size=batch_size, verbose = 1)
        self.model.save(dir + 'Training/SA_Model')
        with open(dir + 'Training/tokenizer.pickle', 'wb') as handle:
            pickle.dump(self.tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)



            
    def predict(self,inp):        #can give a single string as input, returns 0 for negitive and 1 for positive
        inp = [inp]
        inp = self.tokenizer.texts_to_sequences(inp)
        inp = pad_sequences(inp, maxlen=100, dtype='int32', value=0)
        sentiment = self.model.predict(inp,batch_size=1,verbose = 2)[0]
        return np.argmax(sentiment)
    
    def predictByTopic(self,topic,inp):    #Returns if inp is positive or negitive to topic, inp is a string
        inp = inp.split('. ',-1)
        sum = 0
        o = 0
        z = 0
        l = 0
        for s in inp:
            s = s.lower()
            if topic in s:
                l=l+1
                inp = s
                inp = [inp]
                inp = self.tokenizer.texts_to_sequences(inp)
                inp = pad_sequences(inp, maxlen=100, dtype='int32', value=0)
                sentiment = self.model.predict(inp,batch_size=1,verbose = 0)[0]
                sum=sum+sentiment[1]
                if(sentiment[1]<0.5):
                    z=z+1
                else:
                    o=o+1
        try: 
          x = sum/l
        except:
          x = -1
        #print(f'positive sentenses:{o}, negitive sentenses:{z}')
        return x

    def predictByArray(self,topics,inp):
        ans = dict()
        for t in topics:
          ans[t]=self.predictByTopic(t,inp)
        return ans

#Code for testing comment out later
# s = SAModel(True)
# print(s.predict('It is a good movie'))
# print(s.predictByTopic('coffee','Coffee is very good for health. Coffee has many anti-oxidants. I love coffee'))

